= A collection of small example FPGA designs

== Run Vivado as a remote host

This setup is useful if you have a client computer where you want to do local FPGA debugging (i.e. with the physical FPGA plugged in), but a more powerful PC somewhere else where you can run synthesis/implementation.

This setup was tested using Linux Mint 21.1 on both computers, and Vivado 2023.2
installed on both computers.

General information about setting up remote hosts is provided https://docs.xilinx.com/r/en-US/ug904-vivado-implementation/Using-Remote-Hosts-and-Compute-Clusters[here]; however, some details are missing/not explained in sufficient detail, such as exactly how to configure SSH, how to set up mounts, and the https://support.xilinx.com/s/question/0D52E00006iHlI5SAK/lauching-runs-on-a-remote-host-on-ubuntu?language=en_US[dash/bash detail].

Follow the steps below to get set up.

=== Configure SSH

This configuration assumes you are working on a local network, without any firewall rules setup that will interfere with connections. Before you begin, ping the computer you want to use as the remote host from the client using `ping remotehost`.

A few notes:
* Get `remotehost` from your router page.
* If you have a flaky router like I currently have, you can make a hostname based on the IP address by adding an entry like `ip-address remotehost` to your `/etc/hosts` file on the client

Make sure ping works before moving one.

On the computer that will become the remote host, install SSH server by running `sudo apt install openssh-server`. You should then be able to test it by logging in from the client, using your remote-host username and password:

[,bash]
----
# Check this works (password-based login) before moving on
ssh username@remotehost
----

Now that SSH works, you need to configure it so that a password-less login is available for use by Vivado. The way we will do that here is by generating an SSH keypair which does not use a passphrase.

(On a personal network you trust, this is sufficient security. If you need more security, do not follow this approach. Instead, use a passphrase, and then use `ssh-add` every time you want to access the remote host.)

[,bash]
----
# ON CLIENT

# Generate the public/private keypair, do not add a passphrase
cd ~/.ssh
ssh-keygen -t ed25519 -f vivado_key
----

This will create two files: `vivado_key` and `vivado_key.pub`. Leave the first where it is (`~/.ssh`), and move the second to the `~/.ssh` folder on the remote host using any method (`*.pub` is not a private file).

On the remote host, open the `~/.ssh/authorized_keys` file (or create it if it does not exist), and paste the contents of `~/.ssh/vivado_key.pub` to the last line. Then, double check the permissions of the file are correct by running:

[,bash]
----
# ON REMOTE HOST

chmod 640 ~/.ssh/authorized_keys
----

Finally, set up the client so that attempts to ssh into the remote host via its hostname use the right settings. Open `~/.ssh/config` on the client, and place the following contents there:

[,conf]
----
Host remotehost
     IdentitiesOnly yes
     IdentityFile ~/.ssh/vivado_key
     User username
----

Change `username` to your username on the remote host. Now, check that password-less SSH works by running `ssh remotehost`.

You need Vivado to work inside this remote shell, which means sourcing the environment properly. If you are unable to run `vivado` inside the ssh shell, then make sure something like this is added to your `~/.bashrc` on the remote host:

[,bash]
----
# Configure the environment for running Vivado
source /tools/Xilinx/Vivado/2023.2/settings64.sh
----

Close the ssh session and restart it, then verify that `vivado` is now available.

=== Configure Client Shell

Due to an issue with Vivado's scripts, the client shell must be configured to use bash instead of dash (default on Ubuntu) (see https://support.xilinx.com/s/question/0D52E00006iHlI5SAK/lauching-runs-on-a-remote-host-on-ubuntu?language=en_US[here]). You can do this by running:

[,bash]
----
# ON CLIENT

# Pick "No" (i.e. don't use dash as the system shell)
sudo dpkg-reconfigure dash
----

Note that this is on the client computer, not on the remote host.

=== Configure Vivado

Open a Vivado project and click "Run synthesis". Select "Launch runs on remote hosts" and click "Configure Hosts". Inside the settings, click "Manual Configuration". Add a new host, specifying the `remotehost` as the "Name" and choose a number of jobs based on the number of cores on the remote host. Leave the ssh command default, and click "Test". The test should pass.

If the test fails with a message about ping failing, then go back and check that the hostname is correct (try pinging the hostname manually, and check all IP addresses if any are involved). This is a networking issue, not an SSH problem.

If the test fails and ping is not mentioned, double check that the shell is reconfigured to use `bash` as described above.

If the tests continue to fail, ensure that it is possible to login using `ssh remotehost`, and that running `vivado` in this shell works.

=== Configure Mounted Directories

At this point, the connection is set up and ready, but it will still not be possible to run jobs, because the remote host and the local computer must both work from a common working directory.

If you try to synthesize a design without setting this up, it will appear like it is working, but then will hang in the "Queued" state indefinitely, as described https://support.xilinx.com/s/question/0D52E00006txIsESAU/unable-to-start-any-runs-with-remote-host-with-vivado-20212-tasks-remain-queued?language=en_US[here].

From the Xilinx documentation on setting up remote hosts, it states:

"Vivado IDE project files (.xpr) and directories (.data and .runs) must be visible from the mounted file systems on remote machines. If the design data is saved to a local disk, it may not be visible from remote machines."
-- Using Remote Hosts and Compute Clusters, UG904

Interpreting this as directly as possible, it appears to suggest to imply that all the paths must be identical on both the client and the remote host. (If they are not identical, how would the remote host know where to look for the folders?) This means that the folder on both the client or the remote host can be in a user folder (e.g. ~/Documents), since that path depends on the username.

Confirmation that this is the right approach is provided by the log entry `CMD  1: ssh -q -o ConnectTimeout=30 -o ConnectionAttempts=3 -o BatchMode=yes remotehost cd \"/opt/projects/fpga_projects\"; \"/opt/projects/fpga_projects/blinky/blinky.runs/.jobs/job1.sh\"` printed to stdout in the terminal that `vivado` was launched from, after running the remote job. The command attempts to change to the same directory on the server as is used on the client.

NOTE: In this example setup, both the client and the remote host both have the same version of Vivado installed, but the installations are separate (one is not a mounted copy of the other). This makes no difference compared to installing once and using a mounted copy. If this approach is used instead, for consistency with the mounting described here, install Vivado on the server and then mount it on the client.

To test the same-path hypothesis, we will create a folder `/opt/projects`, which will be the location of all project folders. This folder will exist on the remote host, and be mounted on the client. Create it using:

[,bash]
----
# ON REMOTE HOST

# Create the folder, and change ownership
sudo mkdir /opt/projects
sudo chown username:username /opt/projects
----

NOTE: It is important for this folder to be owned by the SSH user, so that Vivado runs inside the remote host can read/write the projects directory.

To mount this folder on the client, use NFS. Assuming as before a trusted private network in which the remote host and client can communicate, with no firewalls in use, the setup is as follows (see https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-22-04[here] for reference)

First, install the NFS server as follows:

[,bash]
----
# ON REMOTE HOST

sudo apt install nfs-kernel-server
----

On the client, you need to install the NFS client:

[,bash]
----
# ON REMOTE HOST

sudo apt install nfs-common
----

To make the `/opt/projects` folder available for the client, open `/etc/exports` on the remote server with sudo, and add the following lines:

[,conf]
----
# Replace the network with the address of your own private network.
# /24 means that any clients with an IP address of 192.168.1.* are allowed.
/opt/projects 192.168.1.0/24(rw,sync,no_subtree_check,all_squash,anonuid=1000,anongid=1000)
----

Specifying `all_squash` will map reads and writes by any user on the client to the specified UID and GID on the host, which we will make match the SSH username. This decouples the username of the account on the client from the user on the remote host.

NOTE: The `anonuid` and `anongid` are the IDs of the SSH user, `username`, on the remote host. Find the numbers by running `id -u username` for the UID, and `id -g username` for the GID. Here, we assume they are 1000.

Save and close the file, and restart NFS using `sudo systemctl restart nfs-kernel-server`.

Now create the mount point on the client: `sudo mkdir /opt/projects`. Ensure that the path is the same, to keep Vivado happy.

The final step is to mount the directory, manually at first, to check it works:

[,bash]
----
# ON CLIENT

mount remotehost:/opt/projects /opt/projects
----

Change into that folder on the client, and run `touch hello`. If this did not give permission-denied issues, then the setup is working. You can check that file exists on the server, and should be owned by `username`, even though it is potentially owned by a different username on the client.

=== Troubleshooting

After completing the setup above, some issues remained. These are explained below.

==== OpenSSL Version Mismatch

First, there appears to be an issue with OpenSSL versions when running on Linux Mint 21.1/21.2. The following message is printed to stdout when attempting to run the remote host runs:

[,bash]
----
CMD  1: ssh -q -o ConnectTimeout=30 -o ConnectionAttempts=3 -o BatchMode=yes remotehost cd \"/home/jrs/Documents/git/rv0\"; \"/opt/projects/fpga_projects/blinky/blinky.runs/.jobs/job6.sh\"
# while {$doneCount<$launchCount} {
#     vwait doneCount ;# Wait for all jobs to finish
# }
  1-> OpenSSL version mismatch. Built against 30000020, you have 30100000
  1 END
----

When running `ssh -V` using the regular Ubuntu terminal, the result is `OpenSSH_8.9p1 Ubuntu-3ubuntu0.6, OpenSSL 3.0.2 15 Mar 2022`, where the OpenSSL version `3.0.2` is presumably the `30000020` in the error. (Assuming this is a client side error because the log shows up directly after running the `ssh` command, which would run on the client side.)

The issue is reproducible by running `ssh` from inside Vivado tcl:

[,bash]
----
# Open Vivado in tcl mode
vivado -mode tcl

# Run SSH from inside Vivado
Vivado% ssh

# WARNING: [Common 17-259] Unknown Tcl command 'ssh -V' sending command to the OS # shell for execution. It is recommended to use 'exec' to send the command to the # OS shell.
# OpenSSL version mismatch. Built against 30000020, you have 30100000
# child process exited abnormally
----

Prepending `exec` reduces the error to `OpenSSL version mismatch. Built against 30000020, you have 30100000`. So the issue is the way that Vivado/TCL interact with ssh. In particular, the Vivado/TCL environment may be using a different version of OpenSSL.

To confirm this, compare the outputs from `openssl version` running in a regular Ubuntu terminal, and running from within Vivado/TCL:

[,bash]
----
# Regular Ubuntu
openssl version
# OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022)

# Vivado/TCL
openssl version
# 3.0.2 15 Mar 2022 (Library: OpenSSL 3.1.0-dev )
# 3.1.0 is the wrong 30100000 library referenced in the error
----

Running `whereis openssl` returns `/usr/bin/openssl` in both Vivado/TCL and the Ubuntu terminal, so the issue is not the binary. To check the libraries that each attempt to link, run `ldd /usr/bin/openssl` from Vivado/TCL and the Ubuntu terminal:

[,bash]
----
# Regular Ubuntu
linux-vdso.so.1 (0x00007ffc8a867000)
libssl.so.3 => /lib/x86_64-linux-gnu/libssl.so.3 (0x00007f0f3594a000)
libcrypto.so.3 => /lib/x86_64-linux-gnu/libcrypto.so.3 (0x00007f0f35400000)
libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f0f35000000)
/lib64/ld-linux-x86-64.so.2 (0x00007f0f35b02000)

# Vivado/TCL
linux-vdso.so.1 (0x00007fff555fe000)
libssl.so.3 => /tools/Xilinx/Vivado/2023.2/tps/lnx64/python-3.8.3/lib/libssl.so.3 (0x00007f042b800000)
libcrypto.so.3 => /tools/Xilinx/Vivado/2023.2/tps/lnx64/python-3.8.3/lib/libcrypto.so.3 (0x00007f042b000000)
libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f042ac00000)
libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f042bb70000)
libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f042bb69000)
libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f042bb64000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f042bca0000)
----

The issue is `libssl`, which is automatically being taken from a Xilinx directory. The reason for the different is that the `LD_LIBRARY_PATH` variable is present _inside_ the Vivado/TCL environment:

[,bash]
----
LD_LIBRARY_PATH=/tools/Xilinx/Vivado/2023.2/tps/lnx64/python-3.8.3/lib:/tools/Xilinx/Vivado/2023.2/tps/lnx64/java-cef-95.0.4638.69/bin/lib/linux64:/tools/Xilinx/Vivado/2023.2/tps/lnx64/javafx-sdk-17.0.1/lib:/tools/Xilinx/Vivado/2023.2/lib/lnx64.o/Default:/tools/Xilinx/Vivado/2023.2/lib/lnx64.o:/tools/Xilinx/Vivado/2023.2/tps/lnx64/jre17.0.7_7/lib/:/tools/Xilinx/Vivado/2023.2/tps/lnx64/jre17.0.7_7/lib//server:/tools/Xilinx/Vivado/2023.2/bin/../lnx64/tools/dot/lib
----

Note that this variable is not sourced by `settings64.sh` (the main environment setup for Vivado) -- it is specifically added for the Vivado/TCL console.

Looking https://askubuntu.com/questions/1438582/how-to-install-openssl-3-0-7-on-ubuntu-22-04[here] shows that in Nov 2022, Ubuntu 22.04 was using OpenSSL version 3.0.2, and that the package version was as follows (pasted from that answer):

[,bash]
----
$ apt list openssl
Listing... Done
openssl/jammy-security,now 3.0.2-0ubuntu1.7 amd64 [installed,automatic]
----

Running this command on Linux Mint Cinnamon 21.1 (based on Ubuntu 22.04) results in:

[,bash]
----
$ apt list openssl
Listing... Done
openssl/jammy-updates,jammy-security,now 3.0.2-0ubuntu1.12 amd64 [installed]
openssl/jammy-updates,jammy-security 3.0.2-0ubuntu1.12 i386
----

This shows that broadly the same version is still in use. The https://docs.xilinx.com/r/en-US/ug973-vivado-release-notes-install-license/Supported-Operating-Systems[official operating system support] for Vivado 2023.2 lists Ubuntu 22.04 LTS (long term support). Testing on Ubuntu 22.04.3 (LTS) gave:

[,bash]
----
$ apt list openssl -a
Listing... Done
openssl/jammy-updates,jammy-security,now 3.0.2-0ubuntu1.10
amd64 [installed,automatic]
openssl/jammy 3.0.2-0ubuntu1 amd64
----

This shows the same version is also in use in the LTS version.

Since the path to the Vivado-packaged OpenSSL library is a python library path `/tools/Xilinx/Vivado/2023.2/tps/lnx64/python-3.8.3/lib/libssl.so.3`, it is possible that a python package is responsible for bringing in the recent OpenSSl version.


===== Potential Solutions

Here are the options:

* Upgrade the Ubuntu SSH and OpenSSL libraries to match those inside the Vivado distribution. This might be tricky, especially given the large number of things that depend on OpenSSL in the Ubuntu distribution. See https://unix.stackexchange.com/questions/753182/is-it-possible-to-get-openssl-3-1-on-ubuntu-22-04[here] for some notes.
* Try to find a way to have Vivado use a specific SSH version which is compatible with the Vivado-packaged OpenSSL version. This approach may be more contained (only requires a new ssh binary somewhere, may not interfere with Ubuntu).
* Try to force the Vivado/TCL environment to link the OS-level OpenSSL library, and not its own packaged version. It may be tricky to achieve this without messing up linking of other packaged libraries that Vivado needs.

The best solution would have been if Vivado had also packaged an `ssh` binary for its own use. This is not the case, because running `sudo apt remove openssh-client`, followed by running `ssh` inside the Vivado/TCL console gives `no such file or directory`.

Going to try removing the Vivado-packaged OpenSSL `libssl.so.3` in the `/tools/Xilinx/Vivado/2023.2/tps/lnx64/python-3.8.3/lib`:

[,bash]
----
cd /tools/Xilinx/Vivado/2023.2/tps/lnx64/python-3.8.3/lib
mv libssl.so.3 old_libssl.so.3
----

Now, (Vivado/TCL) `ldd /usr/bin/openssl` links to `/tools/Xilinx/Vivado/2023.2/lib/lnx64.o/libssl.so.3`, so removed that one too.

[,bash]
----
cd /tools/Xilinx/Vivado/2023.2/lib/lnx64.o
mv libssl.so.3 old_libssl.so.3
----

Now `libssl.so.3` comes from the OS (`/lib/x86_64-linux-gnu/libssl.so.3`), but the OpenSSL version error remains. This is likely due to `libcrypto.so.3`, which is also part of OpenSSL:

[,bash]
----
cd /tools/Xilinx/Vivado/2023.2/tps/lnx64/python-3.8.3/lib
mv libcrypto.so.3 old_libcrypto.so.3
cd /tools/Xilinx/Vivado/2023.2/lib/lnx64.o
mv libcrypto.so.3 old_libcrypto.so.3
----

Now Vivado/TCL `ldd /usr/bin/openssl` does not show any libraries linked from inside the Vivado installation directory:

[,bash]
----
Vivado% exec ldd /usr/bin/openssl
# linux-vdso.so.1 (0x00007ffd4c31f000)
# libssl.so.3 => /lib/x86_64-linux-gnu/libssl.so.3 (0x00007f520e347000)
# libcrypto.so.3 => /lib/x86_64-linux-gnu/libcrypto.so.3 (0x00007f520de00000)
# libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f520da00000)
# /lib64/ld-linux-x86-64.so.2 (0x00007f520e4ff000)
----

Running `ssh` now works, confirming the issue with the libraries. Removing the Vivado-packaged libraries in this way has not solved the problem; now, arbitrary code invoked by Vivado may find an older OpenSSL version than it requires, and raise the error in the other direction. We will cross that bridge when we come to it.

==== Synthesis Failed

In my case, after fixing the OpenSSL error, I progressed from hanging at the `Queued` stage to getting a `Synthesis Failed` message. I found no errors in the stdout in the Vivado terminal, or errors in the messages window.

The first place to check is the `runme.log` file in the `synth_1` folder inside the `.runs` directory:

[,bash]
----
# ON CLIENT OR REMOTE HOST

cd /opt/projects/fpga_projects/blinky/blinky.runs/synth_1
cat runme.log
----

The result was:

----
*** Running vivado
    with args -log top_wrapper.vds -m64 -product Vivado -mode batch -messageDb vivado.pb -notrace -source top_wrapper.tcl

./ISEWrap.sh: 37: vivado: not found
----

Tracing through how the remote call happens

1. The first step is the ssh command listed in the stdout (on the terminal vivado was opened from) when the remote host is invoked: `CMD  1: ssh -q -o ConnectTimeout=30 -o ConnectionAttempts=3 -o BatchMode=yes remotehost cd \"/opt/projects/fpga_projects\"; \"/opt/projects/fpga_projects/blinky/blinky.runs/.jobs/job1.sh\"`
2. Looking at the script `job1.sh`, the variable `HD_LDIR` is set to the directory of the `job1.sh` file, using the `dirname` command. Then, the `runme.sh` script is invoked relative to this directory on line 24.

Manually SSHing into the remote host, changing into the `.jobs` directory and running `job1.sh` directly exits after a few moments with no errors. Looking at the `runme.log`, Vivado is found in this case:

----
*** Running vivado
    with args -log top_wrapper.vds -m64 -product Vivado -mode batch -messageDb vivado.pb -notrace -source top_wrapper.tcl


****** Vivado v2023.2 (64-bit)
  **** SW Build 4029153 on Fri Oct 13 20:13:54 MDT 2023
  **** IP Build 4028589 on Sat Oct 14 00:45:43 MDT 2023
  **** SharedData Build 4025554 on Tue Oct 10 17:18:54 MDT 2023
    ** Copyright 1986-2022 Xilinx, Inc. All Rights Reserved.
    ** Copyright 2022-2023 Advanced Micro Devices, Inc. All Rights Reserved.
----

So the issue is something to do with the way SSH invokes the script. Back on the client computer, run the SSH command as a test:

[,bash]
----
# ON CLIENT

# Remove the escaped characters in the paths
ssh -q -o ConnectTimeout=30 -o ConnectionAttempts=3 -o BatchMode=yes remotehost cd "/opt/projects/fpga_projects"; "/opt/projects/fpga_projects/blinky/blinky.runs/.jobs/job1.sh"
----

It looked like this also worked, which is odd since this is the command the Vivado is also running.
